"use strict";(globalThis.webpackChunkiowarp_site=globalThis.webpackChunkiowarp_site||[]).push([[6091],{3950:(e,t,s)=>{s.r(t),s.d(t,{default:()=>n});s(6540);var i=s(72),r=s(4848);function n(){return(0,r.jsxs)(i.N,{title:"HPC Application Deployment",path:"/research/demos/jarvis-deployment",description:"Reproducible HPC deployment automation in under 2 minutes with 100% success rate. Natural language-driven workflow orchestration.",children:[(0,r.jsx)("h1",{children:"HPC Application Deployment"}),(0,r.jsx)("p",{className:"mb-6 text-lg italic",children:"Reproducible HPC deployment automation in under 2 minutes with 100% success rate"}),(0,r.jsxs)("div",{className:"my-8",children:[(0,r.jsx)("div",{className:"relative",style:{paddingBottom:"56.25%",height:0},children:(0,r.jsx)("iframe",{src:"https://www.youtube.com/embed/ogUpMwEU-SU",title:"HPC Deployment Automation Demo",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0,className:"absolute left-0 top-0 h-full w-full"})}),(0,r.jsx)("p",{className:"mt-2 text-sm italic text-neutral-text-light-muted dark:text-neutral-text-dark-muted",children:"Watch automated HPC deployment orchestration from environment discovery through execution"})]}),(0,r.jsx)("h2",{children:"Overview"}),(0,r.jsxs)("p",{children:["HPC deployment traditionally requires hours spent configuring environments, writing job scripts, and debugging scheduler submissions. IOWarp's Agent Toolkit solves this through ",(0,r.jsx)("strong",{children:"natural language automation"})," where researchers describe what they want and agents handle environment discovery, package resolution, build configuration, job script generation, and execution monitoring. Deployment times range from ",(0,r.jsx)("strong",{children:"25 to 109 seconds"})," depending on agent configuration, with ",(0,r.jsx)("strong",{children:"100% success across 20 diverse prompts"})," in production testing."]}),(0,r.jsx)("p",{children:"The toolkit provides comprehensive HPC deployment capabilities including application deployment (parallel I/O benchmarks, simulations, analysis workflows), system deployment (storage systems, middleware, libraries), environment management (modules, dependencies, compilers), and job orchestration (Slurm, PBS, LSF scheduler integration). Deployments automatically adapt to cluster-specific configurations through per-cluster resource graphs capturing hardware, software, and scheduler details."}),(0,r.jsx)("hr",{}),(0,r.jsx)("h2",{children:"Technical Architecture"}),(0,r.jsxs)("p",{children:["The ",(0,r.jsx)("strong",{children:"IOWarp Agent Toolkit"})," provides intelligent HPC deployment automation through integration with cluster management systems and schedulers. A typical deployment progresses through five phases:"]}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"(1) Environment Discovery"})," \u2014 Agents query cluster configuration via resource graphs, discover available compute nodes and hardware specs, inventory software modules and compilers, and determine scheduler type and configuration. This autonomous reconnaissance generates complete environment documentation without manual system exploration."]}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"(2) Package Resolution"}),' \u2014 The toolkit maps user requests (e.g., "deploy IOR benchmark") to application definitions, resolves dependency graphs (MPI, I/O libraries, profilers), verifies all dependencies are available or can be built, and determines build order for complex multi-component deployments.']}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"(3) Build and Configuration"})," \u2014 Agents load required software modules, compile applications if needed with cluster-specific flags, generate hostfiles for distributed execution, and configure runtime environment variables. All compilation and configuration happens automatically based on cluster characteristics."]}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"(4) Job Script Generation"})," \u2014 The toolkit creates Slurm/PBS/LSF submission scripts, sets appropriate resource requests (nodes, tasks, time limits), configures job output and error logging, and adds application-specific execution commands. Scripts are optimized for each cluster's scheduler and resource policies."]}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"(5) Execution and Monitoring"})," \u2014 Agents submit jobs to the scheduler, track job status (queued, running, completed), collect stdout/stderr output, and gather results for analysis. Complete end-to-end automation from request to results."]}),(0,r.jsx)("hr",{}),(0,r.jsx)("h2",{children:"Results"}),(0,r.jsx)("p",{children:'We evaluated HPC deployment automation by deploying the IOR parallel I/O benchmark using eight different agent-model configurations. Each was tested with five diverse prompts, from terse commands like "Deploy ior with 8 processes" to detailed bash scripts. Testing was conducted on Chameleon Cloud using compute nodes with 40GB A100 GPUs.'}),(0,r.jsx)("img",{src:"/img/demos/jarvis_duration_chart.png",alt:"HPC deployment duration across agent configurations",className:"my-6",loading:"lazy"}),(0,r.jsx)("p",{className:"mb-6 text-sm italic text-neutral-text-light-muted dark:text-neutral-text-dark-muted",children:"Average deployment time across five prompts for eight agent-model combinations"}),(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Configuration"}),(0,r.jsx)("th",{children:"Avg Time"}),(0,r.jsx)("th",{children:"Success Rate"}),(0,r.jsx)("th",{children:"Key Characteristics"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("strong",{children:"OpenCode + Devstral"})}),(0,r.jsx)("td",{children:"24.8s"}),(0,r.jsx)("td",{children:"100%"}),(0,r.jsx)("td",{children:"Local LLM execution, fastest"})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("strong",{children:"Cursor + GPT-4o"})}),(0,r.jsx)("td",{children:"37.7s"}),(0,r.jsx)("td",{children:"85%"}),(0,r.jsx)("td",{children:"Cloud-based, struggles with parameter synonyms"})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("strong",{children:"Gemini CLI + Gemini 2.5 Pro"})}),(0,r.jsx)("td",{children:"85.9s"}),(0,r.jsx)("td",{children:"100%"}),(0,r.jsx)("td",{children:"Robust across variations"})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("strong",{children:"Claude Code + Sonnet 4"})}),(0,r.jsx)("td",{children:"109.2s"}),(0,r.jsx)("td",{children:"100%"}),(0,r.jsx)("td",{children:"Most thorough, 100% reliability"})]})]})]}),(0,r.jsxs)("p",{className:"mt-4",children:["The standout result: ",(0,r.jsx)("strong",{children:"configurations using small local LLMs for execution (like Devstral) matched or beat large cloud models"})," in both speed and accuracy. OpenCode with Devstral completed deployments in 24.8 seconds on average, while Claude Code with Sonnet 4 took 109.2 seconds but achieved perfect reliability. This validates IOWarp's split planning-execution architecture\u2014use powerful models to understand intent, then execute locally for speed, cost reduction, and data security."]}),(0,r.jsx)("img",{src:"/img/demos/jarvis_robustness.png",alt:"HPC deployment robustness across 20 prompt variations",className:"my-6",loading:"lazy"}),(0,r.jsx)("p",{className:"mb-6 text-sm italic text-neutral-text-light-muted dark:text-neutral-text-dark-muted",children:"Configuration success rates across 20 prompts with varying detail and phrasing"}),(0,r.jsxs)("p",{children:['Beyond speed, we tested robustness by varying how users expressed the same deployment intent. Twenty prompts ranged from simple ("Deploy ior with 8 processes using the deployment agent") to explicit parameter specifications to complete bash scripts. Four configurations achieved ',(0,r.jsx)("strong",{children:"perfect 100% success rates"}),": Claude Code + Sonnet 4, Gemini CLI + Gemini 2.5 Pro, OpenCode + Gemini 2.5 Pro, and remarkably, OpenCode + Devstral (the local LLM). This demonstrates that IOWarp successfully abstracts deployment complexity\u2014users can be terse or verbose, use different terminology for the same parameter, and still get working deployments."]}),(0,r.jsx)("img",{src:"/img/demos/jarvis_tool_success.png",alt:"Tool call success rates for HPC deployment",className:"my-6",loading:"lazy"}),(0,r.jsx)("p",{className:"mb-6 text-sm italic text-neutral-text-light-muted dark:text-neutral-text-dark-muted",children:"Success rates for correctly identifying and invoking deployment tools"}),(0,r.jsx)("p",{children:'Beyond deployment success, we measured whether agents correctly identified which tools to invoke. High-performing configurations achieved 95-100% correct tool selection. The occasional errors typically involved parameter name variations ("nprocs" vs "num_processes"), but most agents self-corrected after initial failures, demonstrating the robustness of IOWarp\'s tool integration design.'}),(0,r.jsx)("hr",{}),(0,r.jsx)("h2",{children:"Impact"}),(0,r.jsxs)("p",{children:["IOWarp's Agent Toolkit transforms HPC deployment from an error-prone manual process requiring specialized expertise into a ",(0,r.jsx)("strong",{children:"fast, reproducible, natural language-driven workflow"}),". Researchers deploy complex simulations and benchmarks in under two minutes with perfect reliability. Manual deployment of the same workflow typically requires 30+ minutes for repeat deployments and hours for first-time setup."]}),(0,r.jsxs)("p",{children:["Applications include ",(0,r.jsx)("strong",{children:"materials science workflows"})," orchestrating end-to-end pipelines from manufacturing diagnostics through X-ray microscopy to mechanical testing with ~100GB per specimen, ",(0,r.jsx)("strong",{children:"I/O benchmarking"})," rapidly deploying IOR, VPIC-IO, HACC-IO across different configurations with automated result collection, ",(0,r.jsx)("strong",{children:"simulation campaigns"})," deploying LAMMPS molecular dynamics, WRF weather modeling, or HACC cosmological simulations with reproducible configurations, and ",(0,r.jsx)("strong",{children:"system software testing"})," deploying and validating storage systems across different cluster environments."]}),(0,r.jsxs)("p",{children:["The split planning-execution architecture proves that local LLMs can handle execution while reserving expensive cloud models for planning, achieving the trifecta of speed, cost efficiency, and data security. Beyond individual deployments, this enables ",(0,r.jsx)("strong",{children:"one-command reproducibility"}),": share deployment specifications instead of pages of documentation. New team members deploy working environments immediately. Cross-cluster portability becomes automatic."]}),(0,r.jsx)("hr",{}),(0,r.jsx)("h2",{children:"Learn More"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"/platform/tooling/agent-toolkit",children:"IOWarp Agent Toolkit"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"/research/demos/material-science",children:"Material Science Workflow"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"https://github.com/grc-iit/jarvis-cd",children:"Jarvis GitHub Repository"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"https://github.com/iowarp/iowarp-mcps",children:"IOWarp MCPs"})})]})]})}}}]);