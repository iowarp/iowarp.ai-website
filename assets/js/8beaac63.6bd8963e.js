"use strict";(globalThis.webpackChunkiowarp_site=globalThis.webpackChunkiowarp_site||[]).push([[8522],{7517:(e,t,a)=>{a.r(t),a.d(t,{default:()=>i});a(6540);var n=a(8907),s=a(8774),r=a(4848);function i(){return(0,r.jsx)(n.A,{title:"GPU-First Storage - IOWarp",description:"Intelligent memory tiering for scientific computing workloads",children:(0,r.jsxs)("main",{className:"margin-vert--lg container",children:[(0,r.jsxs)("section",{className:"margin-bottom--lg text--center",children:[(0,r.jsx)("h1",{children:"GPU-First Storage"}),(0,r.jsx)("p",{className:"hero__subtitle",children:"Intelligent Memory Tiering for Scientific Computing"})]}),(0,r.jsxs)("section",{className:"margin-bottom--xl",children:[(0,r.jsx)("h2",{children:"GPU-First Storage: Transparent Memory Tiering"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"Scientific datasets are growing faster than GPU memory. AI training runs crash with out-of-memory errors. Simulations that need 500GB of working data are limited to GPUs with 80GB of HBM. The traditional solution is buying more expensive hardware with larger memory, but this approach doesn't scale. Even the most advanced GPUs can't keep up with the data demands of modern scientific computing."}),(0,r.jsx)("p",{children:"GPU-First Storage takes a different approach. Instead of constraining workloads to fit available GPU memory, it treats the entire storage hierarchy (HBM, DRAM, NVMe) as a unified memory pool. Researchers run their existing code without modification while IOWarp automatically moves data between storage tiers based on access patterns. The system predicts what data will be needed next, prefetches it to faster storage, and evicts cold data to make room. This means a researcher with an 80GB GPU can process a 500GB dataset as if they had infinite memory."})]}),(0,r.jsxs)("section",{className:"margin-bottom--xl",children:[(0,r.jsx)("h2",{children:"Eternia: GPU Memory Extension"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"Eternia extends GPU workloads beyond HBM limits by transparently tiering data across the memory hierarchy. When a deep learning model needs more memory than the GPU provides, Eternia automatically pages tensor data between HBM and DRAM, then to NVMe storage if needed. The system observes GPU access patterns during the first few training iterations to learn which tensors are accessed frequently (weights, activations) versus rarely (gradients that get immediately consumed). Hot data stays in HBM while warm data moves to DRAM and cold data shifts to storage."}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"What makes Eternia unique is that it requires zero code changes. Researchers don't annotate tensors or manage memory manually. The system intercepts GPU memory allocations, tracks access patterns through GPU page faults, and makes tiering decisions automatically. This means existing PyTorch, TensorFlow, or custom CUDA code runs unmodified while benefiting from effectively unlimited GPU memory. On deep learning workloads where datasets are 4x larger than available GPU memory, Eternia achieves 1.8x speedup compared to traditional CPU-based paging, while maintaining near-native performance for data that fits in memory."}),(0,r.jsxs)("div",{className:"text--center margin-vert--lg",children:[(0,r.jsx)("img",{src:"/img/papers/eternia_overview.png",alt:"Eternia Memory Tiering Overview",style:{maxWidth:"800px",width:"100%",backgroundColor:"white",padding:"1rem",borderRadius:"4px"}}),(0,r.jsx)("p",{className:"text--sm text--secondary margin-top--sm",children:"Eternia's transparent memory tiering across HBM, DRAM, and NVMe storage"})]})]}),(0,r.jsxs)("section",{className:"margin-bottom--xl",children:[(0,r.jsx)("h2",{children:"GPU Direct Storage (GDS)"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"Moving data from storage to GPU memory traditionally requires copying through CPU memory first. For a 100GB dataset, this means: NVMe \u2192 CPU DRAM \u2192 PCIe \u2192 GPU HBM. Each copy takes time and consumes PCIe bandwidth. GPU Direct Storage (GDS) eliminates the CPU bottleneck by enabling direct transfers from NVMe storage to GPU memory. Data travels: NVMe \u2192 PCIe \u2192 GPU HBM, cutting the path in half."}),(0,r.jsx)("p",{children:"IOWarp's integration with GDS enables this direct path for scientific workloads. When Eternia needs to page in a tensor from storage, it uses GDS to transfer data directly to GPU memory without CPU involvement. This not only reduces latency but also frees up the CPU to focus on computation rather than memory shuffling. For workloads that frequently access storage (like iterative training on datasets larger than DRAM), GDS provides 2.3x throughput improvement over traditional CPU-mediated transfers, reaching sustained bandwidth of 12 GB/s per GPU."}),(0,r.jsxs)("div",{className:"text--center margin-vert--lg",children:[(0,r.jsx)("img",{src:"/img/papers/gds_cufile_write.png",alt:"GPU Direct Storage Performance",style:{maxWidth:"700px",width:"100%",backgroundColor:"white",padding:"1rem",borderRadius:"4px"}}),(0,r.jsx)("p",{className:"text--sm text--secondary margin-top--sm",children:"Direct GPU-to-NVMe data paths eliminate CPU bottlenecks"})]})]}),(0,r.jsxs)("section",{className:"margin-bottom--xl",children:[(0,r.jsx)("h2",{children:"UnboxKV: LLM Inference Optimization"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"Large language model inference faces a different memory challenge. The KV cache, which stores attention keys and values for previously generated tokens, grows with every token the model produces. For long conversations or document analysis (32K+ tokens), the KV cache can consume more memory than the model weights themselves. Traditional approaches either limit context length or use expensive high-memory GPUs."}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"UnboxKV applies intelligent eviction policies to the KV cache. Not all tokens are equally important for future predictions. The system identifies which past tokens are likely to be referenced by upcoming attention operations and keeps only those in GPU memory. Rarely-accessed tokens move to DRAM or storage. When the model does need an evicted token (rare but possible), UnboxKV fetches it back. This allows serving longer contexts on the same hardware while maintaining generation quality. UnboxKV enables 3.2x longer context windows on the same GPU memory budget with less than 5% degradation in generation quality."}),(0,r.jsxs)("div",{className:"text--center margin-vert--lg",children:[(0,r.jsx)("img",{src:"/img/papers/unboxkv_recomp_vs_swap.png",alt:"UnboxKV Recomputation vs Swapping",style:{maxWidth:"700px",width:"100%",backgroundColor:"white",padding:"1rem",borderRadius:"4px"}}),(0,r.jsx)("p",{className:"text--sm text--secondary margin-top--sm",children:"UnboxKV's intelligent KV cache management for extended context windows"})]})]}),(0,r.jsxs)("section",{className:"margin-bottom--xl",children:[(0,r.jsx)("h2",{children:"Content Transfer Engine: The Tiering Foundation"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"IOWarp's Content Transfer Engine provides the foundational data movement infrastructure that makes GPU-First Storage possible. The engine implements the core tiering logic: monitoring access patterns, predicting future accesses, managing data placement across storage tiers, and handling transfers between HBM, DRAM, and NVMe. This infrastructure isn't specific to one project. Eternia, UnboxKV, and future GPU memory extensions all build on the same underlying framework."}),(0,r.jsx)("p",{children:"For scientific computing users, this means running larger workloads on existing infrastructure. Climate scientists can process higher-resolution simulations without upgrading to 8-GPU nodes. Materials researchers can train larger molecular dynamics models on lab workstations. Genomics teams can run inference on 70B+ parameter models for protein folding analysis. The benefit isn't just cost savings (though avoiding $30K GPU upgrades helps), it's enabling science that wasn't previously possible on available hardware."})]}),(0,r.jsxs)("section",{className:"margin-bottom--lg text--center",children:[(0,r.jsx)("h2",{children:"Learn More"}),(0,r.jsx)("p",{className:"margin-bottom--md",children:"Explore how IOWarp enables intelligent data movement across the memory hierarchy."}),(0,r.jsxs)("div",{children:[(0,r.jsx)(s.A,{className:"button button--primary button--lg margin--sm",to:"/platform/storage",children:"Storage Platform"}),(0,r.jsx)(s.A,{className:"button button--secondary button--lg margin--sm",to:"/docs/getting-started/installation",children:"Get Started"})]})]})]})})}}}]);